
\documentclass{article}

\usepackage{style/conference}
\usepackage{opensans}
\usepackage{graphicx}
\usepackage{biblatex}
\usepackage{fontawesome}
\usepackage[hidelinks]{hyperref}

\addbibresource{references.bib}
\input{style/math_commands.tex}

% replace this title with your own
\title{Winged Horses with an Autoencoder}

\begin{document}
\maketitle
\begin{abstract}
    This paper proposes using an autoencoder to generate images that look like a Pegasus. 
    I train an autoencoder, and condition it on birds and horses, identified by the annotated class labels for each image. 
    This gives two latent codes via the encoder network which I linearly interpolate between to give the final outputs 
    via the decoder network. 
\end{abstract}

\section{Methodology}
The method is to train an autoencoder ~\cite{kramer1991nonlinear}, by minimising the squared L2 loss:
\begin{equation}
    \mathcal{L}_{\textrm{AE}} = \mathbb{E}_{\vx\sim p_{\textrm{data}}}[\, \lVert \vx - D(E(\vx)) \rVert^2 ]
\end{equation}
The encoder function $E : \mathbb{R}^n \rightarrow\mathbb{R}^m$ compresses the dimensionality of the data $n << m$ to a latent encoding 
$z = E(x)$, which is then recovered by the decoder $D: \mathbb{R}^m \rightarrow \mathbb{R}^n$, where $\hat{x} = D(z)$. 
The latent codes are linearly interpolated between to give the final outputs via the decoder network. 
The autoencoder is conditioned on birds and horses which are identified by the annotated class labels available for the images. 

The method was developed on the CIFAR-10 dataset \cite{krizhevsky2009learning} and then tested on the STL-10 dataset \cite{pmlr-v15-coates11a}. 
This is because the images in the CIFAR dataset are only 32x32 pixels, so are much faster to train on, meaning that any errors in the 
method would become apparent much more quickly. 
Adjustments had to be made to the autoencoder to accommodate the change from 32x32 images to 96x96 images, this was the addition of a
convolution block in both the encoding and decoding stages. 
Without this extra block, the output images contained unnaturally large blocks of similarly coloured pixels. 

The autoencoder can be visualised as below: 
\begin{center}
    \includegraphics[width=0.5\textwidth]{figures/architecture.pdf}
\end{center}
Where the neural network is trained to predict its inputs, thus learning an identity which it can reproduce as its output. 
The architectural diagram above was created using Inkscape and exported to a PDF. 

\section{Results}
The method ran for 100 epochs, with a learning rate of 0.001. 
The results look very blurry, where the best batch of images looks like this:
\begin{center}
    \includegraphics[width=0.5\textwidth]{figures/best-batch.png}
\end{center}
From this batch, the most Pegasus-like image (with quite a stretch of the imagination) is:
\begin{center}
    \includegraphics[width=0.075\textwidth]{figures/best-pegasus.png}
\end{center}

\section{Limitations}
It's very difficult to see anything that looks like a Pegasus. 
In the future, this could be improved by training for more than 100 epochs, 
although this was not possible due to the time constraints. 
The majority of the winged horses in the batch are not white, this could be improved by manually selecting white birds 
and white horses from the dataset on which to train, rather than sampling randomly, 
however this was also not possible due to the time constraints. 

\section*{Bonuses}
This submission has a total bonus of +3 marks, as it is trained with STL-10 at the full 96x96 pixels.

% you can have an unlimited number of references (they can go on the 5th page and span many additional pages without any penalty)
\printbibliography
\end{document}
